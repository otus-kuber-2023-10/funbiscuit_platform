# funbiscuit_platform
funbiscuit Platform repository

# ДЗ 1
В kubernetes-intro добавил Dockerfile и nginx.conf для сборки образа веб-сервера, выдающего
файлы из директории /app.

Для запуска веб-сервера сделан манифест пода web-pod.yaml.

Для проекта Hipster Shop сделан манифест пода frontend: frontend-pod.yaml. Для запуска
потребовались дополнительные переменные среды (адреса других микросервисов), их добавил
в манифест frontend-pod-healthy.yaml.

# ДЗ 2
В kubernetes-controllers добавил конфиг для локального кластера kind из 4 нод.
Запустил кластер локально, убедился, что есть 3 worker ноды и одна с ролью control-plane.

Попробовал создать replicaset для запуска frontend микросервиса. Манифест из задания содержал
ошибку - отсутствовал селектор, из-за чего ReplicaSet нельзя создать. Также отсутствовали
настройки переменных среды. После добавления селектора и переменных среды ReplicaSet создался,
а под перешел в состояние Ready.

Увеличил количество реплик до 3 командой kubectl scale, число подов увеличилось до трёх.

После удаления всех подов, они были автоматически созданы вновь благодаря контроллеру ReplicaSet.

Повторно применил манифест с 1 репликой, число подов уменьшилось обратно до одного.

Увеличил число реплик в манифесте до трех и применил его, число подов увеличилось обратно до трех.

Изменил версию образа в манифесте и повторно применил его, но ничего не произошло. Поды остались
работать на прежней версии образа. После удаления всех подов, они были созданы повторно, уже с новой
версией образа. Так происходит, потому что ReplicaSet не сравнивает фактическую спецификацию подов
и свой шаблон. Смотрит только на количество подов, возвращаемых по указанному селектору и создает новые,
когда их не хватает (либо убивает лишние).

Создал манифест replicaset для запуска v0.0.1 версии payment service. Применил и убедился, что сервис
поднялся на трех подах. Удалил replicaset, создал манифест deployment на его основе. Применил deployment.
Убедился, что создались три пода, которыми управляет созданный replicaset.

Обновил в манифесте версию образа на v0.0.2 и применил его. Убедился, что при обновлении применялась
стратегия RollingUpdate. Новые поды создавались по одному, а старые удалялись после каждого запущенного пода.

Выполнил kubectl rollout undo для deployment и откатил к предыдущей версии. На изначальном replicaset произошло
масштабирование до трех реплик, а на новом - до 0.

Создал deployment для сценария развертывания blue-green указанием maxSurge 3 и maxUnavailable 0. Таким образом
контроллер deployment сначала создаст replicaset для новой версии и запустит в нем все три реплики (благодаря maxSurge)
и только после запуска новых реплик начнет удалять старые поды.

Создал deployment для сценария развертывания reverse rolling update указанием maxSurge 0 и maxUnavailable 1. Таким образом
контроллер deployment сначала в существующем replicaset уменьшит число реплик на 1, затем увеличит в новом на 1.
И так до полного перехода на новую версию.

Создал deployment для запуска сервиса frontend и добавил в него readinessProbe. Применил deployment и убедился, что
все поды запустили и перешли в Ready.

Изменил в deployment readinessProbe на некорректный и запустил обновление на версию v0.0.2. Убедился, что при обновлении
запустился один под и дальше обновление не шло, т.к. этот под не мог перейти в Ready (readinessProbe возвращал ошибку)

Создал daemon set для запуска node exporter на всех четырех нодах. Для запуска в control plane указад в блоке
tolerations те taints, которыми отмечены ноды control-plane и master, чтобы node-exporter в том числе запустился там.
Через kubectl describe pod убедился, что поды запущенны на разных нодах (т.е. на всех четырех, включая control plane).
Пробросил порт через kubectl port-forward и убедился, что метрики выводятся.
